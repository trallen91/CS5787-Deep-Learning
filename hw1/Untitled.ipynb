{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    train = np.loadtxt(\"iris-train.txt\")\n",
    "    train_y = train[:,0]\n",
    "    train_x = train[:, [1,2]]\n",
    "    \n",
    "    test = np.loadtxt(\"iris-test.txt\")\n",
    "    test_y = test[:,0]\n",
    "    test_x = test[:, [1,2]]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "iris_train_x, iris_train_y, iris_test_x, iris_test_y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_dp_matrix(data, weight_matrix):  \n",
    "    return np.transpose(np.dot(np.transpose(weight_matrix), np.transpose(data)))\n",
    "\n",
    "def softmax_vector(x):\n",
    "    e_x = np.exp(x - np.max(x)) #subtracting the max makes it more stable #https://deepnotes.io/softmax-crossentropy \n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax_matrix(matrix, num_classes):\n",
    "    N = matrix.shape[0]\n",
    "    flat_output_matrix = np.array([])\n",
    "    for data_point in matrix:\n",
    "        flat_output_matrix = np.append(flat_output_matrix, softmax_vector(data_point))\n",
    "        \n",
    "    return flat_output_matrix.reshape(N, num_classes)\n",
    "# def calc_dp_vector(data_point, weight_matrix):\n",
    "#     return np.dot(np.transpose(weight_matrix), data_point)\n",
    "\n",
    "# dp_vec = calc_dp_vector(train_x[0], weights)\n",
    "\n",
    "# def forward_prop(data, current_weights):\n",
    "#     # Step 1: Calculate dot product of data and weights\n",
    "#     matrix_dot_product = calc_dp_matrix(data, current_weights)\n",
    "#     # Step 2: Perform softmax to get probabilities for each class\n",
    "#     return softmax_matrix(matrix_dot_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds = np.argmax(sample_output, axis=1) +\n",
    "\n",
    "def indices(l, val):\n",
    "    retval = []\n",
    "    last = 0\n",
    "    while val in l[last:]:\n",
    "           i = l[last:].index(val)\n",
    "           retval.append(last + i)\n",
    "           last += i + 1   \n",
    "    return retval\n",
    "\n",
    "def class_accuracy(y_pred, y_true, class_num):\n",
    "    y_pred = list(y_pred)\n",
    "    y_true = list(y_true)\n",
    "    index = indices(y_true, class_num)\n",
    "#     print(index)\n",
    "    y_pred, y_true = [y_pred[i] for i in index], [y_true[i] for i in index]\n",
    "    tp = [1 for k in range(len(y_pred)) if y_true[k]==y_pred[k]]\n",
    "    tp = np.sum(tp)\n",
    "    return tp/float(len(y_pred))\n",
    "\n",
    "def mean_per_class_accuracy(y_pred, y_true):\n",
    "    return (class_accuracy(y_pred, y_true,1) + class_accuracy(y_pred, y_true,2) + class_accuracy(y_pred, y_true,3))/3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_classifier(train_x, train_y, test_x, test_y, learning_rate, weight_decay, num_classes):\n",
    "    NUM_EPOCHS = 10000\n",
    "       \n",
    "    mean_training_features = np.mean(train_x, axis=0)\n",
    "    train_x_norm = train_x - mean_training_features\n",
    "    test_x_norm = test_x - mean_training_features \n",
    "    \n",
    "    NUM_FEATURES = train_x_norm.shape[1]\n",
    "#     NUM_CLASSES = num_classes\n",
    "    \n",
    "    TRAIN_SAMPLE_SIZE = train_x_norm.shape[0]\n",
    "\n",
    "    #MAKE ONE HOT LABELS\n",
    "    one_hot_labels_train = np.zeros((TRAIN_SAMPLE_SIZE, num_classes))\n",
    "\n",
    "    for i in range(TRAIN_SAMPLE_SIZE):  \n",
    "        one_hot_labels_train[i, train_y[i] - 1] = 1\n",
    "\n",
    "    one_hot_flat_train = one_hot_labels_train.reshape(1, one_hot_labels_train.shape[0] * one_hot_labels_train.shape[1])\n",
    "    \n",
    "    TEST_SAMPLE_SIZE = test_x_norm.shape[0]\n",
    "\n",
    "    one_hot_labels_test = np.zeros((TEST_SAMPLE_SIZE, num_classes))\n",
    "\n",
    "    for i in range(TEST_SAMPLE_SIZE):  \n",
    "        one_hot_labels_test[i, test_y[i] - 1] = 1\n",
    "\n",
    "    one_hot_flat_test = one_hot_labels_test.reshape(1, one_hot_labels_test.shape[0] * one_hot_labels_test.shape[1])\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    weights = np.random.rand(NUM_FEATURES, num_classes)\n",
    "    \n",
    "    train_loss_array = []\n",
    "    test_loss_array = []\n",
    "\n",
    "    train_accuracy_arr = []\n",
    "    test_accuracy_arr = []\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):  \n",
    "        train_data = train_x_norm\n",
    "        test_data = test_x_norm\n",
    "\n",
    "        #FORWARD PROP:\n",
    "        # Step 1: Calculate dot product of data and weights\n",
    "        train_matrix_dot_product = calc_dp_matrix(train_data, weights)\n",
    "\n",
    "        test_matrix_dot_product = calc_dp_matrix(test_data, weights)\n",
    "\n",
    "        # Step 2: Perform softmax to get probabilities for each class\n",
    "        train_activation_output = softmax_matrix(train_matrix_dot_product, num_classes)\n",
    "\n",
    "        test_activation_output = softmax_matrix(test_matrix_dot_product, num_classes)\n",
    "\n",
    "        #ACCURACY:\n",
    "        train_preds = np.argmax(train_activation_output, axis=1) + 1\n",
    "        train_mean_acc = mean_per_class_accuracy(train_preds, train_y)\n",
    "        train_accuracy_arr.append(train_mean_acc)\n",
    "\n",
    "        test_preds = np.argmax(test_activation_output, axis=1) + 1\n",
    "        test_mean_acc = mean_per_class_accuracy(test_preds, test_y)\n",
    "        test_accuracy_arr.append(test_mean_acc)\n",
    "\n",
    "        #BACK PROP\n",
    "        # Step 1: Compute Loss\n",
    "        train_sm_flat = train_activation_output.reshape(train_activation_output.shape[0]*train_activation_output.shape[1],1)\n",
    "        train_loggify_sm = np.log(train_sm_flat)   \n",
    "        train_loss = (-1 * np.dot(one_hot_flat_train, train_loggify_sm)[0][0]) / train_data.shape[0]\n",
    "        train_loss_array.append(train_loss)\n",
    "\n",
    "        test_sm_flat = test_activation_output.reshape(test_activation_output.shape[0]*test_activation_output.shape[1],1)\n",
    "        test_loggify_sm = np.log(test_sm_flat)   \n",
    "        test_loss = (-1 * np.dot(one_hot_flat_test, test_loggify_sm)[0][0]) / test_data.shape[0]\n",
    "        test_loss_array.append(test_loss)\n",
    "\n",
    "        #Step 2: Find gradient \n",
    "        error_matrix = train_activation_output - one_hot_labels_train\n",
    "        weight_diff = weight_decay*weights + np.transpose(np.dot(np.transpose(error_matrix), train_data))\n",
    "\n",
    "        weights -= (learning_rate * weight_diff)\n",
    "        \n",
    "    return train_loss_array, test_loss_array, train_accuracy_arr, test_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/travisallen/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/travisallen/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:26: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "iris_train_loss, iris_test_loss, iris_train_accuracy, iris_test_accuracy = softmax_classifier(iris_train_x, iris_train_y, iris_test_x, iris_test_y, .07, .01, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(iris_train_loss)\n",
    "plt.plot(iris_test_loss)\n",
    "\n",
    "plt.legend(['training loss', 'test loss'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iris_train_accuracy)\n",
    "plt.plot(iris_test_accuracy)\n",
    "plt.legend(['training mean accuracy', 'test mean accuracy'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean_training_features = np.mean(iris_train_x, axis=0)\n",
    "\n",
    "# train_x_norm = iris_train_x - mean_training_features\n",
    "# test_x_norm = iris_test_x - mean_training_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NUM_CLASSES = 3\n",
    "# NUM_FEATURES = train_x_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN_SAMPLE_SIZE = train_x_norm.shape[0]\n",
    "\n",
    "# #MAKE ONE HOT LABELS\n",
    "# one_hot_labels_train = np.zeros((TRAIN_SAMPLE_SIZE, NUM_CLASSES))\n",
    "\n",
    "# for i in range(TRAIN_SAMPLE_SIZE):  \n",
    "#     one_hot_labels_train[i, iris_train_y[i] - 1] = 1\n",
    "\n",
    "# one_hot_flat_train = one_hot_labels_train.reshape(1, one_hot_labels_train.shape[0] * one_hot_labels_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST_SAMPLE_SIZE = test_x_norm.shape[0]\n",
    "\n",
    "# one_hot_labels_test = np.zeros((TEST_SAMPLE_SIZE, NUM_CLASSES))\n",
    "\n",
    "# for i in range(TEST_SAMPLE_SIZE):  \n",
    "#     one_hot_labels_test[i, iris_test_y[i] - 1] = 1\n",
    "\n",
    "# one_hot_flat_test = one_hot_labels_test.reshape(1, one_hot_labels_test.shape[0] * one_hot_labels_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #PARAMETERS\n",
    "# lr = .07\n",
    "# l2 = .01\n",
    "# NUM_EPOCHS = 1000\n",
    "\n",
    "# np.random.seed(42)\n",
    "# weights = np.random.rand(NUM_FEATURES, NUM_CLASSES) # initizialize m * K weight matrix\n",
    "\n",
    "# train_loss_array = []\n",
    "# test_loss_array = []\n",
    "\n",
    "# train_accuracy_arr = []\n",
    "# test_accuracy_arr = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):  \n",
    "#     train_data = train_x_norm\n",
    "#     test_data = test_x_norm\n",
    "    \n",
    "#     #FORWARD PROP:\n",
    "#     # Step 1: Calculate dot product of data and weights\n",
    "#     train_matrix_dot_product = calc_dp_matrix(train_data, weights)\n",
    "    \n",
    "#     test_matrix_dot_product = calc_dp_matrix(test_data, weights)\n",
    "    \n",
    "#     # Step 2: Perform softmax to get probabilities for each class\n",
    "#     train_activation_output = softmax_matrix(train_matrix_dot_product, NUM_CLASSES)\n",
    "    \n",
    "#     test_activation_output = softmax_matrix(test_matrix_dot_product,NUM_CLASSES)\n",
    "\n",
    "#     #ACCURACY:\n",
    "#     train_preds = np.argmax(train_activation_output, axis=1) + 1\n",
    "#     train_mean_acc = mean_per_class_accuracy(train_preds, iris_train_y)\n",
    "#     train_accuracy_arr.append(train_mean_acc)\n",
    "    \n",
    "#     test_preds = np.argmax(test_activation_output, axis=1) + 1\n",
    "#     test_mean_acc = mean_per_class_accuracy(test_preds, iris_test_y)\n",
    "#     test_accuracy_arr.append(test_mean_acc)\n",
    "    \n",
    "#     #BACK PROP\n",
    "#     # Step 1: Compute Loss\n",
    "#     train_sm_flat = train_activation_output.reshape(train_activation_output.shape[0]*train_activation_output.shape[1],1)\n",
    "#     train_loggify_sm = np.log(train_sm_flat)   \n",
    "#     train_loss = (-1 * np.dot(one_hot_flat_train, train_loggify_sm)[0][0]) / train_data.shape[0]\n",
    "#     train_loss_array.append(train_loss)\n",
    "    \n",
    "#     test_sm_flat = test_activation_output.reshape(test_activation_output.shape[0]*test_activation_output.shape[1],1)\n",
    "#     test_loggify_sm = np.log(test_sm_flat)   \n",
    "#     test_loss = (-1 * np.dot(one_hot_flat_test, test_loggify_sm)[0][0]) / test_data.shape[0]\n",
    "#     test_loss_array.append(test_loss)\n",
    "    \n",
    "#     #Step 2: Find gradient \n",
    "#     error_matrix = train_activation_output - one_hot_labels_train\n",
    "#     weight_diff = l2*weights + np.transpose(np.dot(np.transpose(error_matrix), train_data))\n",
    "    \n",
    "#     weights -= (lr * weight_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# plt.plot(train_loss_array)\n",
    "# plt.plot(test_loss_array)\n",
    "\n",
    "# plt.legend(['training loss', 'test loss'], loc='upper right')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(train_accuracy_arr)\n",
    "# plt.plot(test_accuracy_arr)\n",
    "# plt.legend(['training mean accuracy', 'test mean accuracy'], loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X, y = train_x, train_y\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "# clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "# clf.fit(X, y)\n",
    "\n",
    "def plot_decision_boundary(pred_func):\n",
    "\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "plot_decision_boundary(lambda x: my_predict())\n",
    "\n",
    "# # clf.predict(X)\n",
    "\n",
    "def my_predict(X):\n",
    "    return train_preds\n",
    "\n",
    "# my_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadCIFAR10():\n",
    "    def unpickle(file):\n",
    "        import cPickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "        return dict\n",
    "    \n",
    "    train_batch_1 = unpickle(\"./CIFAR_DATA/data_batch_1\")\n",
    "    train_batch_2 = unpickle(\"./CIFAR_DATA/data_batch_2\")\n",
    "    train_batch_3 = unpickle(\"./CIFAR_DATA/data_batch_3\")\n",
    "    train_batch_4 = unpickle(\"./CIFAR_DATA/data_batch_4\")\n",
    "    train_batch_5 = unpickle(\"./CIFAR_DATA/data_batch_5\")\n",
    "        \n",
    "    trainFeat = np.concatenate((train_batch_1['data'], train_batch_2['data'], train_batch_3['data'], train_batch_4['data'], train_batch_5['data']))\n",
    "    trainLabels = np.concatenate((train_batch_1['labels'], train_batch_2['labels'], train_batch_3['labels'], train_batch_4['labels'], train_batch_5['labels']))\n",
    "    \n",
    "    test_batch = unpickle(\"./CIFAR_DATA/test_batch\")\n",
    "    \n",
    "    testFeat = test_batch['data']\n",
    "    testLabels = test_batch['labels']\n",
    "    \n",
    "    return trainLabels, trainFeat, testLabels, testFeat\n",
    "    \n",
    "    \n",
    "trainLabels, trainFeat, testLabels, testFeat = loadCIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_first_three():\n",
    "    \n",
    "#     def plot_image(img):\n",
    "#         blah = trainFeat[0].reshape(32,32,3)\n",
    "#         plt.imshow(blah)\n",
    "    \n",
    "    counter = 0\n",
    "    for label in range(0,10):\n",
    "        print label\n",
    "        for idx, train_label in enumerate(trainLabels):\n",
    "            if counter == 3:\n",
    "                counter = 0\n",
    "                break\n",
    "            if(train_label == label):\n",
    "                plt.imshow(trainFeat[idx].reshape(32,32,3))\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "\n",
    "print_first_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([1,3,2,2,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp([17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp(17)/(np.exp(17)+np.exp(39)+np.exp(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp(39)/(np.exp(17)+np.exp(39)+np.exp(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp(40)/(np.exp(17)+np.exp(39)+np.exp(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp([17,39,40])/(np.exp(17) + np.exp(39)+np.exp(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS CALCULATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax_output_dummy = np.array([[.5, .4, .1], [.2, .1, .7]])\n",
    "one_hot_label_dummy = np.array([[0,1,0], [0,0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_rs = one_hot_label_dummy.reshape(1, one_hot_label_dummy.shape[0] * one_hot_label_dummy.shape[1])\n",
    "sm_rs = softmax_output_dummy.reshape(softmax_output_dummy.shape[0]*softmax_output_dummy.shape[1],1)\n",
    "loggify_sm = np.log(sm_rs)\n",
    "loss = -1 * np.dot(oh_rs, loggify_sm)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_rs = softmax_output_dummy.reshape(softmax_output_dummy.shape[0]*softmax_output_dummy.shape[1],1)\n",
    "sm_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loggify_sm = np.log(sm_rs)\n",
    "loggify_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-1 * np.dot(oh_rs, loggify_sm)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
